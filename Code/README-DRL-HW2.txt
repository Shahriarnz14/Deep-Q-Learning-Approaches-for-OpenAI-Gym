===================================================================
===================================================================
			    README:
===================================================================
Authors:
Shahriar Noroozizadeh
Saeed Moayedpour
===================================================================
===================================================================
++++++++++++++++++++++++++++(QUESTION 2)+++++++++++++++++++++++++++
===================================================================
===================================================================
FILES:
===================================================================
example.py : Example Running Policy/Value Iteration
rl.py      : (A)Sync Policy/Value Iteration Implementations
===================================================================
INSTRUCTIONS:
===================================================================
To get the results we obtained you can simply run "example.py"
It will prompt you to input a number from 0 - 7 corresponding
to different method to run the MDP.

0: Random Policy, 
1: Policy-Iter, 
2: Value-Iter, 
3: Policy-Iter-Order, 
4: Policy-Iter-Rnd
5: Value-Iter-Order, 
6: Value-Iter-Rnd 
7: Custom (Manhattan-Distance)
===================================================================
EXTRA NOTE:
===================================================================
The way files are referenced are such that "rl.py" file should
be inside the "deeprl_hw2q2" folder, but "example.py" should
be in the same directory that the folder exists (NOT inside).

===================================================================
===================================================================
++++++++++++++++++++++++++++(QUESTION 3)+++++++++++++++++++++++++++
===================================================================
===================================================================
Files: 
===================================================================
DQN_Implementation_Final.py         : DQN Implementation
DQN_Implementation_Double_Final.py  : Double-DQN Implementation
===================================================================
RUN EXAMPLE:
===================================================================
python DQN_Implementation_Final.py --env CartPole-v0
python DQN_Implementation_Final.py --env MountainCar-v0
python DQN_Implementation_Double_Final.py --env CartPole-v0
python DQN_Implementation_Double_Final.py --env MountainCar-v0
===================================================================
INSTRUCTIONS:
===================================================================
When you run the script, it will automatically create an instance
of QAgent and trains the agent for the given environment.
The maximum number of episodes is set as default in the code to be
10,000 episodes. If you wish to change this number feel free to 
do so in the main function in files.  (LINE:386)

The main hyper-parameters are all stored as class field/attributes
in the python files. The constructors of QNetwork and QAgent
classes have all these values stored and they can be directly 
changed there.

For the Cartpole environment a 1-hidden layer network is used.
For the Mountain Car environment a 3 hidden layers network is used.

===================================================================
NOTE ON EXTRA FILES GENERATED BY THE SCRIPT:
===================================================================
Running the scripts will save Network Model and Network Model 
Weights every 100 episodes (Everytime evaluation happens).
The purpose for this was to be able to later on render the video
on the python console. 
In addition, every 500 episodes two *.npy files will be saved to 
back-up TD-Loss and evaluated test rewards.
===================================================================
NOTE ON SAVING TRAINING VIDEO:
===================================================================
The scripts do not generate video by themselves. Even though the
functionality is still there. We saved the Q-Network weights
and at the end loaded the weights into a new network and manually
saved the video by calling the save_video() method in the Python
console.
===================================================================
			    END of README
===================================================================
===================================================================


